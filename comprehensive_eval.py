#!/usr/bin/env python3
"""
Script ƒë√°nh gi√° to√†n di·ªán cho c√°c model v·ªõi metrics:
- HitRate@1, HitRate@5, HitRate@10
- MRR@1, MRR@5, MRR@10
- Recall@1, Recall@5, Recall@10

Quy tr√¨nh ƒë√°nh gi√°:
- Query: C√°c ·∫£nh g·ªëc trong t·∫≠p test.
- Corpus: To√†n b·ªô ·∫£nh g·ªëc v√† ·∫£nh ƒë√£ augment c·ªßa t·∫≠p train, val, v√† test.
- Ground Truth: V·ªõi m·ªói query (·∫£nh test g·ªëc), k·∫øt qu·∫£ ƒë√∫ng l√† 3 phi√™n b·∫£n augment t∆∞∆°ng ·ª©ng c·ªßa ch√≠nh n√≥ trong corpus.
"""

import yaml
import torch
import torch.utils.data
import pandas as pd
from pathlib import Path
import argparse
import json
from torchvision import transforms

from src.data_loader import create_dataloaders
from src.model_factory import build_model
from src.utils import set_seed, setup_logging
import torch.nn.functional as F

# --- C√°c h√†m t√≠nh to√°n Metrics ---
# Logic c·ªßa c√°c h√†m n√†y ƒë√£ ƒë∆∞·ª£c l√†m g·ªçn l·∫°i ƒë·ªÉ t·∫≠p trung v√†o m·ª•c ti√™u ch√≠nh:
# t√¨m ch√≠nh x√°c c√°c phi√™n b·∫£n augment, thay v√¨ fallback v·ªÅ so s√°nh class label.

def calculate_metrics_with_topk(query_embeddings: torch.Tensor,
                               corpus_embeddings: torch.Tensor,
                               k_values: list,
                               query_to_augmented_mapping: dict) -> dict:
    """
    T√≠nh to√°n HitRate@k, MRR@k v√† Recall@k.

    Args:
        query_embeddings: Embeddings c·ªßa c√°c ·∫£nh test g·ªëc (queries).
        corpus_embeddings: Embeddings c·ªßa to√†n b·ªô ·∫£nh trong corpus.
        k_values: Danh s√°ch c√°c gi√° tr·ªã k (v√≠ d·ª•: [1, 5, 10]).
        query_to_augmented_mapping: Dict map t·ª´ index c·ªßa query ƒë·∫øn list c√°c index c·ªßa
                                    phi√™n b·∫£n augment t∆∞∆°ng ·ª©ng trong corpus.
    """
    results = {}
    
    # Chu·∫©n h√≥a embeddings ƒë·ªÉ t√≠nh cosine similarity
    query_embeddings = F.normalize(query_embeddings, p=2, dim=1)
    corpus_embeddings = F.normalize(corpus_embeddings, p=2, dim=1)
    
    # T√≠nh ma tr·∫≠n t∆∞∆°ng ƒë·ªìng (cosine similarity) gi·ªØa queries v√† corpus
    similarity_matrix = torch.mm(query_embeddings, corpus_embeddings.t())
    
    n_queries = query_embeddings.size(0)
    
    # L·∫•y top-k indices cho t·∫•t c·∫£ c√°c query c√πng m·ªôt l√∫c ƒë·ªÉ tƒÉng hi·ªáu qu·∫£
    # L·∫•y top-k l·ªõn nh·∫•t ƒë·ªÉ t√°i s·ª≠ d·ª•ng cho c√°c k nh·ªè h∆°n
    max_k = max(k_values)
    _, top_k_indices_all = torch.topk(similarity_matrix, max_k, dim=1)

    for k in k_values:
        # L·∫•y top-k cho gi√° tr·ªã k hi·ªán t·∫°i
        top_k_indices = top_k_indices_all[:, :k]
        
        # --- T√≠nh HitRate@k ---
        hit_count = 0
        for i in range(n_queries):
            query_augmented_indices = set(query_to_augmented_mapping.get(i, []))
            retrieved_indices = set(top_k_indices[i].tolist())
            
            # Giao c·ªßa hai t·∫≠p h·ª£p kh√¥ng r·ªóng nghƒ©a l√† ƒë√£ t√¨m th·∫•y √≠t nh·∫•t 1 ground truth
            if not query_augmented_indices.isdisjoint(retrieved_indices):
                hit_count += 1
        
        results[f"HitRate@{k}"] = hit_count / n_queries
        
        # --- T√≠nh MRR@k ---
        reciprocal_ranks = []
        for i in range(n_queries):
            query_augmented_indices = query_to_augmented_mapping.get(i, [])
            best_rank = float('inf')
            
            # T√¨m rank (v·ªã tr√≠) c·ªßa ground truth ƒë·∫ßu ti√™n ƒë∆∞·ª£c t√¨m th·∫•y
            for rank, retrieved_idx in enumerate(top_k_indices[i].tolist()):
                if retrieved_idx in query_augmented_indices:
                    best_rank = rank + 1
                    break
            
            if best_rank != float('inf'):
                reciprocal_ranks.append(1.0 / best_rank)
            else:
                reciprocal_ranks.append(0.0)
        
        results[f"MRR@{k}"] = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0
        
        # --- T√≠nh Recall@k ---
        recall_scores = []
        for i in range(n_queries):
            query_augmented_indices = set(query_to_augmented_mapping.get(i, []))
            retrieved_indices = set(top_k_indices[i].tolist())
            
            # T√≠nh s·ªë l∆∞·ª£ng ground truth ƒë∆∞·ª£c t√¨m th·∫•y
            found_gt = len(query_augmented_indices.intersection(retrieved_indices))
            total_gt = len(query_augmented_indices)
            
            # Recall = s·ªë ground truth t√¨m th·∫•y / t·ªïng s·ªë ground truth
            recall = found_gt / total_gt if total_gt > 0 else 0.0
            recall_scores.append(recall)
        
        results[f"Recall@{k}"] = sum(recall_scores) / len(recall_scores) if recall_scores else 0.0
    
    return results

# --- C√°c h√†m tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng ---

def extract_features(model, dataloader, device):
    """Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho c√°c ·∫£nh g·ªëc (kh√¥ng augment)."""
    model.eval()
    all_features = []
    all_labels = []
    
    with torch.no_grad():
        for images, targets in dataloader:
            images = images.to(device)
            features = model.get_features(images)
            all_features.append(features.cpu())
            all_labels.append(targets.cpu())
    
    if not all_features:
        return None, None
        
    return torch.cat(all_features, dim=0), torch.cat(all_labels, dim=0)

def get_strong_augmentation_transform(image_size=224, backbone='dinov2'):
    """T·∫°o transform v·ªõi augmentation m·∫°nh."""
    
    # Determine normalization parameters based on backbone
    if backbone == 'ent_vit':
        # EndoViT-specific normalization parameters
        mean = [0.3464, 0.2280, 0.2228]
        std = [0.2520, 0.2128, 0.2093]
    else:
        # Standard ImageNet normalization for other models
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
    
    # G·ª£i √Ω: S·ª≠ d·ª•ng torchvision.transforms.v2 ƒë·ªÉ c√≥ th·ªÉ ch·∫°y augment tr√™n GPU, tƒÉng t·ªëc ƒë·ªô ƒë√°ng k·ªÉ
    return transforms.Compose([
        # B∆∞·ªõc 1: Ti·ªÅn x·ª≠ l√Ω - T·∫≠p trung v√†o v√πng quan tr·ªçng (v√≤ng tr√≤n n·ªôi soi)
        # Crop ph·∫ßn trung t√¢m ƒë·ªÉ lo·∫°i b·ªè ph·∫ßn l·ªõn vi·ªÅn ƒëen, gi·∫£ s·ª≠ v√≤ng tr√≤n ·ªü gi·ªØa.
        # ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc crop cho ph√π h·ª£p v·ªõi ·∫£nh c·ªßa b·∫°n.
        transforms.CenterCrop(size=(450, 450)), # Gi·∫£ s·ª≠ ·∫£nh g·ªëc ~500x500
        transforms.Resize((image_size, image_size)), # Resize v·ªÅ k√≠ch th∆∞·ªõc chu·∫©n

        # B∆∞·ªõc 2: Augmentation h√¨nh h·ªçc (M√¥ ph·ªèng chuy·ªÉn ƒë·ªông c·ªßa ·ªëng soi)
        # √Åp d·ª•ng m·ªôt trong c√°c ph√©p bi·∫øn ƒë·ªïi h√¨nh h·ªçc m·ªôt c√°ch ng·∫´u nhi√™n
        transforms.RandomApply([
            transforms.RandomAffine(
                degrees=20,               # Xoay m·ªôt g√≥c h·ª£p l√Ω
                translate=(0.1, 0.1),     # D·ªãch chuy·ªÉn nh·∫π
                scale=(0.9, 1.1)          # Zoom v√†o/ra m·ªôt ch√∫t
                # Shear (bi·∫øn d·∫°ng tr∆∞·ª£t) th∆∞·ªùng kh√¥ng th·ª±c t·∫ø v·ªõi ·ªëng soi, n√™n b·ªè
            )
        ], p=0.7), # √Åp d·ª•ng v·ªõi x√°c su·∫•t 70%

        # transforms.RandomHorizontalFlip(p=0.5), # R·∫•t quan tr·ªçng, m√¥ ph·ªèng soi tai tr√°i/ph·∫£i

        # B∆∞·ªõc 3: Augmentation m√†u s·∫Øc (M√¥ ph·ªèng ƒëi·ªÅu ki·ªán √°nh s√°ng v√† camera kh√°c nhau)
        # S·ª≠ d·ª•ng ColorJitter v·ªõi c∆∞·ªùng ƒë·ªô v·ª´a ph·∫£i
        transforms.ColorJitter(
            brightness=0.2,   # ƒêi·ªÅu ch·ªânh ƒë·ªô s√°ng
            contrast=0.2,     # ƒêi·ªÅu ch·ªânh ƒë·ªô t∆∞∆°ng ph·∫£n
            saturation=0.2,   # ƒêi·ªÅu ch·ªânh ƒë·ªô b√£o h√≤a
            hue=0.05          # HUE r·∫•t nh·∫°y, ch·ªâ n√™n thay ƒë·ªïi r·∫•t √≠t
        ),
        
        # C√°c ph√©p bi·∫øn ƒë·ªïi m√†u s·∫Øc an to√†n kh√°c
        transforms.RandomAutocontrast(p=0.2), # T·ª± ƒë·ªông tƒÉng c∆∞·ªùng ƒë·ªô t∆∞∆°ng ph·∫£n

        # B∆∞·ªõc 4: Augmentation m√¥ ph·ªèng nhi·ªÖu v√† che khu·∫•t
        # L√†m m·ªù nh·∫π ƒë·ªÉ m√¥ ph·ªèng ·∫£nh b·ªã out-focus
        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),

        # Chuy·ªÉn sang Tensor TR∆Ø·ªöC khi th·ª±c hi·ªán RandomErasing
        transforms.ToTensor(),

        # X√≥a m·ªôt v√πng nh·ªè ƒë·ªÉ m√¥ ph·ªèng b·ªã che khu·∫•t (v√≠ d·ª•: b·ªüi r√°y tai)
        transforms.RandomErasing(
            p=0.2, # √Åp d·ª•ng v·ªõi x√°c su·∫•t th·∫•p
            scale=(0.02, 0.08), # X√≥a m·ªôt v√πng nh·ªè
            ratio=(0.3, 3.3),
            value='random' # ƒêi·ªÅn v√†o b·∫±ng nhi·ªÖu ng·∫´u nhi√™n thay v√¨ m√†u ƒëen
        ),
        transforms.Normalize(mean=mean, std=std)
    ])

def extract_augmented_features(model, dataloader, device, backbone, num_augmentations=3):
    """
    Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho c√°c phi√™n b·∫£n augment c·ªßa ·∫£nh.
    H√†m n√†y ch·ªâ tr·∫£ v·ªÅ features c·ªßa c√°c ·∫£nh ƒë√£ augment.
    """
    model.eval()
    all_features = []
    all_labels = []

    if num_augmentations <= 0:
        return torch.tensor
    
    image_size = model.image_size if hasattr(model, 'image_size') else 224
    strong_transform = get_strong_augmentation_transform(image_size, backbone)
    
    # L·∫•y transform chu·∫©n ƒë·ªÉ denormalize ·∫£nh tr∆∞·ªõc khi augment
    if backbone == 'ent_vit':
        # EndoViT-specific normalization parameters
        mean = [0.3464, 0.2280, 0.2228]
        std = [0.2520, 0.2128, 0.2093]
    else:
        # Standard ImageNet normalization for other models
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
    
    denormalize = transforms.Normalize(
        mean=[-mean[0]/std[0], -mean[1]/std[1], -mean[2]/std[2]],
        std=[1/std[0], 1/std[1], 1/std[2]]
    )

    with torch.no_grad():
        for images, targets in dataloader:
            # `images` l√† batch ·∫£nh g·ªëc t·ª´ dataloader
            batch_size = images.size(0)
            
            # L·∫∑p l·∫°i targets cho c√°c phi√™n b·∫£n augment
            augmented_targets = targets.repeat_interleave(num_augmentations)
            all_labels.append(augmented_targets.cpu())

            # T·∫°o v√† x·ª≠ l√Ω c√°c phi√™n b·∫£n augment
            batch_augmented_features = []
            for _ in range(num_augmentations):
                augmented_batch_pil = []
                for i in range(batch_size):
                    img_tensor = images[i].cpu()
                    img_denormalized = denormalize(img_tensor)
                    img_pil = transforms.ToPILImage()(img_denormalized)
                    augmented_batch_pil.append(strong_transform(img_pil))

                augmented_batch_tensor = torch.stack(augmented_batch_pil).to(device)
                features = model.get_features(augmented_batch_tensor)
                batch_augmented_features.append(features)
            
            # N·ªëi c√°c features augment theo ƒë√∫ng th·ª© t·ª±:
            # [img1_aug1, img2_aug1, ..., img1_aug2, img2_aug2, ...]
            # C·∫ßn s·∫Øp x·∫øp l·∫°i ƒë·ªÉ th√†nh:
            # [img1_aug1, img1_aug2, ..., img2_aug1, img2_aug2, ...]
            reordered_features = torch.cat(batch_augmented_features, dim=0).reshape(num_augmentations, batch_size, -1).transpose(0, 1).reshape(batch_size * num_augmentations, -1)
            all_features.append(reordered_features.cpu())

    if not all_features:
        return None, None

    return torch.cat(all_features, dim=0), torch.cat(all_labels, dim=0)


def evaluate_model(config_path, checkpoint_path=None, model_name=""):
    """
    H√†m ch√≠nh ƒë·ªÉ ƒë√°nh gi√° m·ªôt model.
    Quy tr√¨nh ƒë√£ ƒë∆∞·ª£c l√†m r√µ v√† logic ƒë∆∞·ª£c ƒë∆°n gi·∫£n h√≥a.
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # --- 1. T·∫£i Dataloaders v√† Model ---
    train_loader, val_loader, test_loader = create_dataloaders(config['data'], config['model']['backbone'])
    model = build_model(config['model'])
    model.to(device)
    
    if checkpoint_path and Path(checkpoint_path).exists():
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"‚úÖ Loaded checkpoint: {checkpoint_path}")
    else:
        print("‚ÑπÔ∏è No checkpoint loaded. Using pretrained weights from model definition.")
    
    # --- 2. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng ---
    print("\n--- Feature Extraction ---")
    # Query: Test g·ªëc
    print("üìä Extracting features from test set (Queries)...")
    query_features, query_labels = extract_features(model, test_loader, device)
    
    # Corpus Part 1: ·∫¢nh g·ªëc
    print("üìä Extracting features from train set (Corpus - Original)...")
    train_features, train_labels = extract_features(model, train_loader, device)
    print("üìä Extracting features from val set (Corpus - Original)...")
    val_features, val_labels = extract_features(model, val_loader, device)
    
    # Corpus Part 2: ·∫¢nh augment
    num_augmentations = 5
    backbone = config['model']['backbone']
    print(f"üìä Extracting {num_augmentations} augmented features from train set (Corpus - Augmented)...")
    train_aug_features, train_aug_labels = extract_augmented_features(model, train_loader, device, backbone, 1)
    print(f"üìä Extracting {num_augmentations} augmented features from val set (Corpus - Augmented)...")
    val_aug_features, val_aug_labels = extract_augmented_features(model, val_loader, device, backbone, 1)
    print(f"üìä Extracting {num_augmentations} augmented features from test set (Corpus - Ground Truth)...")
    test_aug_features, test_aug_labels = extract_augmented_features(model, test_loader, device, backbone, num_augmentations)

    if query_features is None or train_features is None or test_aug_features is None:
        print(f"‚ùå Failed to extract necessary features for {model_name}. Skipping evaluation.")
        return None

    # --- 3. X√¢y d·ª±ng Corpus v√† Ground Truth Mapping ---
    print("\n--- Building Corpus & Ground Truth ---")
    
    # N·ªëi t·∫•t c·∫£ c√°c features l·∫°i ƒë·ªÉ t·∫°o th√†nh corpus ho√†n ch·ªânh
    corpus_features = torch.cat([
        train_features, 
        val_features,
        train_aug_features,
        val_aug_features,
        test_aug_features
    ], dim=0)
    
    # (T√πy ch·ªçn) N·ªëi labels n·∫øu c·∫ßn debug
    corpus_labels = torch.cat([
        train_labels,
        val_labels,
        train_aug_labels,
        val_aug_labels,
        test_aug_labels
    ], dim=0)
    
    # T√≠nh to√°n v·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa c√°c ·∫£nh test augment trong corpus
    # ƒê√¢y l√† th√¥ng tin c·ªët l√µi ƒë·ªÉ x√°c ƒë·ªãnh ground truth
    test_aug_start_idx = len(train_features) + len(val_features) + len(train_aug_features) + len(val_aug_features)
    
    # T·∫°o mapping t·ª´ query (test g·ªëc) ƒë·∫øn c√°c phi√™n b·∫£n augment c·ªßa n√≥
    query_to_augmented_mapping = {}
    for query_idx in range(len(query_features)):
        start = test_aug_start_idx + query_idx * num_augmentations
        end = start + num_augmentations
        query_to_augmented_mapping[query_idx] = list(range(start, end))

    print(f"  - Total corpus size: {corpus_features.shape[0]} samples")
    print(f"  - Test augmented (ground truth) start index: {test_aug_start_idx}")
    print(f"  - Example mapping: Query 0 -> Corpus indices {query_to_augmented_mapping.get(0)}")

    # --- 4. Debug & Sanity Check (Quan tr·ªçng) ---
    # Ki·ªÉm tra xem feature c·ªßa ·∫£nh g·ªëc c√≥ "g·∫ßn" v·ªõi feature c·ªßa c√°c b·∫£n augment kh√¥ng.
    # Ch√∫ng kh√¥ng bao gi·ªù "b·∫±ng nhau" (equal) do c√≥ ph√©p augment ng·∫´u nhi√™n.
    # Ta k·ª≥ v·ªçng cosine similarity s·∫Ω cao.
    print("\n--- Sanity Check: Similarity of Query vs. its Augmentations ---")
    for i in range(min(3, len(query_features))):
        query_emb = F.normalize(query_features[i:i+1], p=2, dim=1)
        
        aug_indices = query_to_augmented_mapping[i]
        aug_embs = F.normalize(corpus_features[aug_indices], p=2, dim=1)
        
        similarities = torch.mm(query_emb, aug_embs.t())
        avg_sim = similarities.mean().item()
        
        # Ki·ªÉm tra top similarities v·ªõi to√†n b·ªô corpus
        all_sims = torch.mm(query_emb, F.normalize(corpus_features, p=2, dim=1).t())
        top_sim_values, top_sim_indices = torch.topk(all_sims, 10, dim=1)
        
        print(f"  - Query {i} vs. its {num_augmentations} augments - Avg similarity: {avg_sim:.4f}")
        print(f"    Individual similarities: {similarities.squeeze().tolist()}")
        print(f"    Top 10 corpus similarities: {top_sim_values.squeeze()[:5].tolist()}")
        print(f"    Ground truth indices: {aug_indices}")
        print(f"    Top 10 retrieved indices: {top_sim_indices.squeeze()[:5].tolist()}")
        
        # Ki·ªÉm tra xem c√≥ ground truth n√†o trong top 10 kh√¥ng
        gt_in_top10 = any(idx in aug_indices for idx in top_sim_indices.squeeze()[:10].tolist())
        print(f"    Ground truth in top 10: {gt_in_top10}")
        print()
    # --- 5. T√≠nh to√°n v√† Tr·∫£ v·ªÅ k·∫øt qu·∫£ ---
    print("\n--- Calculating Metrics ---")
    metrics = calculate_metrics_with_topk(
        query_features, 
        corpus_features, 
        k_values=[1, 5, 10],
        query_to_augmented_mapping=query_to_augmented_mapping
    )
    
    print(f"\nüìä Results for {model_name}:")
    for metric, value in metrics.items():
        print(f"  - {metric}: {value:.4f}")
    
    return metrics

def create_summary_table(results):
    """T·∫°o b·∫£ng t·ªïng k·∫øt k·∫øt qu·∫£."""
    print("\n" + "="*25 + " SUMMARY TABLE " + "="*25)
    
    rows = []
    for model_name, model_results in results.items():
        if model_results.get('pretrained'):
            for metric, value in model_results['pretrained'].items():
                rows.append({'Model': model_name, 'Training': 'Pretrained', 'Metric': metric, 'Value': value})
        
        if model_results.get('finetuned'):
            for metric, value in model_results['finetuned'].items():
                rows.append({'Model': model_name, 'Training': 'Fine-tuned', 'Metric': metric, 'Value': value})
    
    if not rows:
        print("No results to display.")
        return
        
    df = pd.DataFrame(rows)
    
    # Pivot ƒë·ªÉ c√≥ b·∫£ng so s√°nh tr·ª±c quan
    pivot_df = df.pivot_table(index=['Metric', 'Model'], columns='Training', values='Value')
    
    # S·∫Øp x·∫øp l·∫°i th·ª© t·ª± metric cho d·ªÖ ƒë·ªçc
    metric_order = ['HitRate@1', 'HitRate@5', 'HitRate@10', 'MRR@1', 'MRR@5', 'MRR@10', 'Recall@1', 'Recall@5', 'Recall@10']
    pivot_df = pivot_df.reindex(metric_order, level='Metric')
    
    print(pivot_df.to_string(float_format="%.4f"))
    print("="*65)

def main():
    parser = argparse.ArgumentParser(description='Comprehensive evaluation of image retrieval models')
    parser.add_argument('--output', '-o', default='evaluation_results.json', help='Output file for results in JSON format')
    args = parser.parse_args()
    
    setup_logging()
    set_seed(42)
    
    # ƒê·ªãnh nghƒ©a c√°c model c·∫ßn ƒë√°nh gi√°
    models_to_evaluate = [
        {
            'name': 'DINOv2-ViT-S/14',
            'config': 'configs/dinov2_vits14.yaml',
            'checkpoint': 'outputs/dinov2_vits14_ntxent/best_model2.pth'
        },
        {
            'name': 'ENT-ViT',
            'config': 'configs/ent-vit.yaml',
            'checkpoint': 'outputs/ent_vit_ntxent/best_model2.pth'
        },
        #         {
        #     'name': 'DINOv2-ViT-B/14',
        #     'config': 'configs/dinov2_vitb14.yaml',
        #     'checkpoint': 'outputs/dinov2_vitb14_ntxent/best_model2.pth'
        # },
        #         {
        #     'name': 'DINOv2-ViT-L/14',
        #     'config': 'configs/dinov2_vitl14.yaml',
        #     'checkpoint': 'outputs/dinov2_vitl14_ntxent/best_model2.pth'
        # },
    ]
    
    all_results = {}
    
    print("üöÄ Starting Comprehensive Evaluation...")
    print("=" * 80)
    
    for model_info in models_to_evaluate:
        model_name = model_info['name']
        config_path = Path(model_info['config'])
        checkpoint_path = Path(model_info['checkpoint'])
        
        print(f"\n\nüîç Evaluating Model: {model_name}")
        print("-" * 50)
        
        # ƒê√°nh gi√° model ƒë√£ fine-tune
        print(f"üéØ Evaluating {model_name} (Fine-tuned)")
        finetuned_results = evaluate_model(
            config_path,
            checkpoint_path=checkpoint_path,
            model_name=f"{model_name} (Fine-tuned)"
        )
        
        # ƒê√°nh gi√° model pretrained (kh√¥ng load checkpoint)
        print(f"\nüì¶ Evaluating {model_name} (Pretrained only)")
        pretrained_results = evaluate_model(
            config_path, 
            checkpoint_path=None, 
            model_name=f"{model_name} (Pretrained)"
        )
        
        all_results[model_name] = {
            'pretrained': pretrained_results,
            'finetuned': finetuned_results
        }
    
    # L∆∞u k·∫øt qu·∫£
    output_path = Path(args.output)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=4, ensure_ascii=False)
    
    print(f"\n\nüíæ All evaluation results saved to: {output_path}")
    
    # T·∫°o b·∫£ng t·ªïng k·∫øt
    create_summary_table(all_results)

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Evaluation interrupted by user.")
    except Exception as e:
        print(f"\n‚ùå An unexpected error occurred: {e}")
        import traceback
        traceback.print_exc()

