# Multi-GPU Training Guide

## ğŸš€ Cáº¥u hÃ¬nh Multi-GPU Training

### Kiá»ƒm tra há»‡ thá»‘ng
```bash
# Kiá»ƒm tra GPU vÃ  kháº£ nÄƒng multi-GPU
python check_gpu.py
```

### Cháº¡y training vá»›i multi-GPU

#### 1. Cháº¡y config cá»¥ thá»ƒ
```bash
# Cháº¡y DinoV2 ViT-B/14
bash train_multi_gpu.sh dinov2_vitb14

# Cháº¡y DinoV2 ViT-L/14
bash train_multi_gpu.sh dinov2_vitl14

# Cháº¡y DinoV2 ViT-S/14
bash train_multi_gpu.sh dinov2_vits14

# Cháº¡y ENT-ViT
bash train_multi_gpu.sh ent-vit
```

#### 2. Cháº¡y táº¥t cáº£ config
```bash
# Cháº¡y táº¥t cáº£ config láº§n lÆ°á»£t
bash train_multi_gpu.sh all
```

#### 3. Cháº¡y interactive mode
```bash
# Cháº¡y vÃ  chá»n config
bash train_multi_gpu.sh
```

## âš™ï¸ Cáº¥u hÃ¬nh Ä‘Ã£ tá»‘i Æ°u cho Multi-GPU

### DinoV2 ViT-B/14
- **Batch size**: 16 (8 per GPU)
- **Learning rate**: 0.0002
- **Num workers**: 8
- **Features**: 768D

### DinoV2 ViT-L/14
- **Batch size**: 12 (6 per GPU)
- **Learning rate**: 0.00015
- **Num workers**: 8
- **Features**: 1024D

### DinoV2 ViT-S/14
- **Batch size**: 20 (10 per GPU)
- **Learning rate**: 0.00025
- **Num workers**: 8
- **Features**: 384D

### ENT-ViT
- **Batch size**: 16 (8 per GPU)
- **Learning rate**: 0.0002
- **Num workers**: 8
- **Features**: 768D

## ğŸ”§ TÃ­nh nÄƒng Multi-GPU

### 1. DataParallel
- Tá»± Ä‘á»™ng chia batch cho cÃ¡c GPU
- Synchronize gradients
- Load balancing

### 2. Synchronized Batch Normalization
- Äá»“ng bá»™ statistics giá»¯a cÃ¡c GPU
- Cáº£i thiá»‡n stability cho training

### 3. Automatic batch size adjustment
- Tá»± Ä‘á»™ng chia batch size cho cÃ¡c GPU
- Giá»¯ nguyÃªn total batch size

## ğŸ“Š Monitoring

### Weights & Biases
- Tá»± Ä‘á»™ng log metrics
- Model gradients tracking
- Real-time visualization

### GPU Utilization
```bash
# Monitor GPU usage
watch -n 1 nvidia-smi
```

## ğŸš¨ Troubleshooting

### CUDA Out of Memory
```bash
# Giáº£m batch size trong config
batch_size: 8  # Thay vÃ¬ 16
```

### Single GPU fallback
Náº¿u chá»‰ cÃ³ 1 GPU, script sáº½ tá»± Ä‘á»™ng chuyá»ƒn sang single GPU mode.

### CPU fallback
Náº¿u khÃ´ng cÃ³ GPU, script sáº½ cháº¡y trÃªn CPU (cháº­m hÆ¡n).

## ğŸ“ Logs vÃ  Outputs

### Cáº¥u trÃºc thÆ° má»¥c
```
outputs/
â”œâ”€â”€ dinov2_vitb14_ntxent/
â”‚   â”œâ”€â”€ best_model.pth
â”‚   â””â”€â”€ checkpoint_epoch_*.pth
â”œâ”€â”€ dinov2_vitl14_ntxent/
â”œâ”€â”€ dinov2_vits14_ntxent/
â””â”€â”€ ent_vit_ntxent/
```

### Training logs
- Console output vá»›i progress bars
- File logs: `training.log`
- W&B dashboard: Real-time metrics

## ğŸ” Performance Tips

1. **Batch size**: TÄƒng batch size Ä‘á»ƒ táº­n dá»¥ng multi-GPU
2. **Learning rate**: TÄƒng learning rate tÆ°Æ¡ng á»©ng vá»›i sá»‘ GPU
3. **Num workers**: TÄƒng num_workers cho data loading
4. **SyncBN**: Báº­t synchronized batch norm cho model lá»›n
5. **Memory**: Monitor GPU memory usage
