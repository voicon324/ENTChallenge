#!/usr/bin/env python3
"""
Script chÃ­nh Ä‘á»ƒ cháº¡y training - Ráº¤T Gá»ŒN
Chá»‰ cáº§n cháº¡y: python train.py
"""

import yaml
import wandb
import torch
from pathlib import Path

from src.data_loader import create_dataloaders
from src.model_factory import build_model
from src.trainer import Trainer
from src.utils import set_seed, setup_logging

def main():
    # 1. Táº£i cáº¥u hÃ¬nh tá»« file YAML
    config_path = Path('config.yaml')
    if not config_path.exists():
        raise FileNotFoundError("File config.yaml khÃ´ng tá»“n táº¡i!")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # 2. Setup logging vÃ  seed
    setup_logging()
    set_seed(42)
    
    # 3. Khá»Ÿi táº¡o Weights & Biases
    # W&B sáº½ tá»± Ä‘á»™ng Ä‘á»c config vÃ  lÆ°u láº¡i má»i thá»©
    run = wandb.init(
        project=config['wandb']['project'],
        entity=config['wandb']['entity'],
        name=config['wandb'].get('run_name'),
        config=config,
        job_type="training",
        resume="allow"
    )
    
    # Láº¥y láº¡i config tá»« W&B Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n
    cfg = wandb.config
    
    print(f"ğŸš€ Báº¯t Ä‘áº§u thá»±c nghiá»‡m vá»›i backbone: {cfg['model']['backbone']}")
    print(f"ğŸ“Š Theo dÃµi táº¡i: {run.url}")
    
    # 4. Kiá»ƒm tra GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» Sá»­ dá»¥ng device: {device}")
    
    # 5. Táº¡o DataLoaders
    print("ğŸ“‚ Táº¡o DataLoaders...")
    training_strategy = cfg['training']['strategy']
    backbone = cfg['model']['backbone']
    
    if training_strategy == 'contrastive':
        from src.data_loader import create_contrastive_dataloaders
        train_loader, val_loader = create_contrastive_dataloaders(cfg['data'], backbone)
        # For contrastive learning, we'll use the same val_loader for testing
        test_loader = val_loader
    elif training_strategy == 'ntxent':
        from src.data_loader import create_ntxent_dataloaders
        train_loader, val_loader = create_ntxent_dataloaders(cfg['data'], backbone)
        # For NT-Xent learning, we'll use the same val_loader for testing
        test_loader = val_loader
    else:
        train_loader, val_loader, test_loader = create_dataloaders(cfg['data'], backbone)
    
    # 6. XÃ¢y dá»±ng Model
    print("ğŸ—ï¸ XÃ¢y dá»±ng Model...")
    model = build_model(cfg['model'])
    
    # 7. [W&B] Theo dÃµi model (gradients, network topology)
    if cfg['logging']['log_gradients']:
        wandb.watch(model, log='all', log_freq=cfg['logging']['log_frequency'])
    
    # 8. Khá»Ÿi táº¡o Trainer vÃ  báº¯t Ä‘áº§u huáº¥n luyá»‡n
    print("ğŸ¯ Khá»Ÿi táº¡o Trainer...")
    trainer = Trainer(model, train_loader, val_loader, test_loader, cfg, run)
    
    # 9. Thá»±c hiá»‡n training dá»±a trÃªn strategy
    if cfg['training']['strategy'] == "test_only":
        print("ğŸ” Chá»‰ thá»±c hiá»‡n Ä‘Ã¡nh giÃ¡...")
        trainer.evaluate_test()
    else:
        print("ğŸ”¥ Báº¯t Ä‘áº§u huáº¥n luyá»‡n...")
        trainer.train()
    
    # 10. Káº¿t thÃºc run W&B
    print("âœ… HoÃ n thÃ nh!")
    run.finish()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ Dá»«ng training bá»Ÿi ngÆ°á»i dÃ¹ng")
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        raise
