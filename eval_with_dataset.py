#!/usr/bin/env python3
"""
Script Ä‘Ã¡nh giÃ¡ model sá»­ dá»¥ng bá»™ data evaluation Ä‘Ã£ táº¡o sáºµn.
"""

import yaml
import torch
import argparse
import json
from pathlib import Path
import pandas as pd

from eval_dataset_loader import EvaluationDataset
from src.model_factory import build_model
from src.utils import set_seed, setup_logging


def extract_features_from_images(model, images, device, batch_size=32):
    """
    TrÃ­ch xuáº¥t features tá»« tensor áº£nh.
    
    Args:
        model: Model Ä‘á»ƒ trÃ­ch xuáº¥t features
        images: Tensor áº£nh (N, C, H, W)
        device: Device Ä‘á»ƒ cháº¡y
        batch_size: Batch size
        
    Returns:
        Features tensor
    """
    model.eval()
    all_features = []
    
    with torch.no_grad():
        for i in range(0, len(images), batch_size):
            batch = images[i:i+batch_size].to(device)
            features = model.get_features(batch)
            all_features.append(features.cpu())
    
    return torch.cat(all_features, dim=0)


def evaluate_model_with_dataset(config_path, eval_data_dir, checkpoint_path=None, model_name=""):
    """
    ÄÃ¡nh giÃ¡ model sá»­ dá»¥ng bá»™ data evaluation.
    
    Args:
        config_path: ÄÆ°á»ng dáº«n config model
        eval_data_dir: ThÆ° má»¥c chá»©a bá»™ data evaluation
        checkpoint_path: ÄÆ°á»ng dáº«n checkpoint (optional)
        model_name: TÃªn model
        
    Returns:
        Dict chá»©a káº¿t quáº£ evaluation
    """
    # Load config
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load model
    print(f"ğŸ”„ Loading model: {model_name}")
    model = build_model(config['model'])
    model.to(device)
    
    # Load checkpoint if provided
    if checkpoint_path and Path(checkpoint_path).exists():
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… Loaded checkpoint: {checkpoint_path}")
    else:
        print("â„¹ï¸ No checkpoint loaded. Using pretrained weights.")
    
    # Load evaluation dataset
    print(f"ğŸ“Š Loading evaluation dataset from: {eval_data_dir}")
    eval_dataset = EvaluationDataset(eval_data_dir)
    
    # Load images
    print("ğŸ“· Loading query images...")
    query_images, query_paths = eval_dataset.load_query_images(normalize=True)
    
    print("ğŸ“· Loading corpus images...")
    corpus_images, corpus_paths = eval_dataset.load_corpus_images(normalize=True)
    
    print(f"   Query images: {query_images.shape}")
    print(f"   Corpus images: {corpus_images.shape}")
    
    # Extract features
    print("ğŸ”„ Extracting features from queries...")
    query_features = extract_features_from_images(model, query_images, device)
    
    print("ğŸ”„ Extracting features from corpus...")
    corpus_features = extract_features_from_images(model, corpus_images, device)
    
    print(f"   Query features: {query_features.shape}")
    print(f"   Corpus features: {corpus_features.shape}")
    
    # Evaluate
    print("ğŸ“Š Evaluating retrieval performance...")
    results = eval_dataset.evaluate_retrieval(query_features, corpus_features)
    
    # Print results
    eval_dataset.print_results(results, f"Results for {model_name}")
    
    return results


def main():
    parser = argparse.ArgumentParser(description='Evaluate model using pre-created evaluation dataset')
    parser.add_argument('--config', '-c', required=True, help='Path to model config file')
    parser.add_argument('--eval_data', '-e', default='eval_data', help='Path to evaluation dataset directory')
    parser.add_argument('--checkpoint', '-ckpt', help='Path to model checkpoint (optional)')
    parser.add_argument('--model_name', '-n', default='Model', help='Model name for display')
    parser.add_argument('--output', '-o', help='Output JSON file for results (optional)')
    
    args = parser.parse_args()
    
    setup_logging()
    set_seed(42)
    
    print("ğŸš€ Starting model evaluation...")
    print("=" * 60)
    print(f"ğŸ“ Config: {args.config}")
    print(f"ğŸ“ Eval data: {args.eval_data}")
    print(f"ğŸ“ Checkpoint: {args.checkpoint}")
    print(f"ğŸ·ï¸  Model name: {args.model_name}")
    print("=" * 60)
    
    try:
        # Run evaluation
        results = evaluate_model_with_dataset(
            config_path=args.config,
            eval_data_dir=args.eval_data,
            checkpoint_path=args.checkpoint,
            model_name=args.model_name
        )
        
        # Save results if output path provided
        if args.output:
            output_path = Path(args.output)
            evaluation_results = {
                'model_name': args.model_name,
                'config_path': args.config,
                'checkpoint_path': args.checkpoint,
                'eval_data_dir': args.eval_data,
                'results': results
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ Results saved to: {output_path}")
        
        print("\nğŸ‰ Evaluation completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ Error during evaluation: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
